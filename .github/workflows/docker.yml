name: Docker CI/CD


on:
  push:
    branches:
      - main
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'scripts/**'
      - 'infra/**'
      - 'Dockerfile'
      - 'requirements.txt'
      - 'config.py'
      - 'docker-compose.yml'
      - '.github/workflows/docker.yml'
  workflow_dispatch:

env:
  IMAGE_NAME: sp500-etl
  BUILD_CONTEXT: .
  DOCKERFILE_PATH: ./Dockerfile

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build Docker image
        run: |
          unset DOCKER_CONTEXT || true

          docker build \
            -f ${{ env.DOCKERFILE_PATH }} \
            -t ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest \
            ${{ env.BUILD_CONTEXT }}

      - name: Push Docker image
        run: |
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest

  deploy-to-ec2:
    name: Deploy to EC2
    needs: build-and-push
    runs-on: ubuntu-latest

    env:
      AWS_DB_USER: ${{ vars.AWS_DB_USER }}
      AWS_DB_PASSWORD: ${{ vars.AWS_DB_PASSWORD }}
      AWS_DB_HOST: ${{ vars.AWS_DB_HOST }}
      AWS_DB_PORT: ${{ vars.AWS_DB_PORT }}
      AWS_DB_NAME: ${{ vars.AWS_DB_NAME }}
      AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ vars.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "${{ secrets.EC2_HOST }}" >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Deploy latest image on EC2
        run: |
          ssh ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            set -e

            # 1) Carpeta base
            sudo mkdir -p /opt/sp500
            cd /opt/sp500

            # 2) Asegurar Docker
            if ! sudo systemctl is-active --quiet docker; then
              sudo systemctl start docker || true
            fi

            # 3) Red de Docker para que Airflow vea a Postgres por nombre
            sudo docker network create airflow-net 2>/dev/null || true

            # 4) Postgres para Airflow (si no existe, lo creo; si ya está, lo reutilizo)
            if ! sudo docker ps -a --format '{{.Names}}' | grep -q '^airflow-postgres$'; then
              sudo docker run -d \
                --name airflow-postgres \
                --network airflow-net \
                --restart unless-stopped \
                -e POSTGRES_USER=airflow \
                -e POSTGRES_PASSWORD=airflow \
                -e POSTGRES_DB=airflow \
                -v airflow_pgdata:/var/lib/postgresql/data \
                -p 5432:5432 \
                postgres:15
            else
              if ! sudo docker ps --format '{{.Names}}' | grep -q '^airflow-postgres$'; then
                sudo docker start airflow-postgres
              fi
            fi

            # 5) Detener y borrar contenedor anterior si existe
            if sudo docker ps -a --format '{{.Names}}' | grep -q '^sp500-etl$'; then
              echo "Encontrado contenedor anterior sp500-etl, deteniendo y eliminando..."
              sudo docker stop sp500-etl || true
              sudo docker rm sp500-etl || true
            fi

            # 6) Borrar imagen anterior para liberar espacio (si existe)
            if sudo docker images --format '{{.Repository}}:{{.Tag}}' | grep -q '^${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest$'; then
              echo "Eliminando imagen anterior ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest para liberar espacio..."
              sudo docker rmi ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest || true
            fi

            # 7) Limpiar imágenes y contenedores no usados (sin tocar volúmenes de datos)
            sudo docker container prune -f || true
            sudo docker image prune -f || true

            # 8) Pull de la última imagen del ETL
            echo "Haciendo pull de la nueva imagen..."
            sudo docker pull ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest

            # 9) URL de conexión de Airflow a Postgres (dentro de la red airflow-net)
            AIRFLOW_DB_URI="postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow"
            echo "Usando AIRFLOW_DB_URI=${AIRFLOW_DB_URI}"

            # 10) Migrar / inicializar la DB de Airflow (idempotente)
            sudo docker run --rm \
              --network airflow-net \
              -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
              ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest \
              airflow db migrate || true

            # 11) Levantar Airflow webserver con CloudWatch Logs y la DB correcta
              sudo docker run -d \
                --name sp500-etl \
                --network airflow-net \
                --restart unless-stopped \
                -p 8080:8080 \
                --log-driver awslogs \
                --log-opt awslogs-region=${AWS_DEFAULT_REGION} \
                --log-opt awslogs-group=/sp500/etl \
                --log-opt awslogs-stream=sp500-etl-webserver \
                -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$AIRFLOW_DB_URI" \
                -e AWS_DB_USER=${AWS_DB_USER} \
                -e AWS_DB_PASSWORD=${AWS_DB_PASSWORD} \
                -e AWS_DB_HOST=${AWS_DB_HOST} \
                -e AWS_DB_PORT=${AWS_DB_PORT} \
                -e AWS_DB_NAME=${AWS_DB_NAME} \
                -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
                -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
                -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
                -e S3_BUCKET=${S3_BUCKET} \
                -e ENVIRONMENT=prod \
                ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest webserver
          EOF